"""
Student Performance ML Pipeline
--------------------------------------------------------------------------
Bu script, Ã¶ÄŸrenci verilerini analiz eder, temizler ve hem regresyon hem de 
sÄ±nÄ±flandÄ±rma modelleri kullanarak sÄ±nav baÅŸarÄ±sÄ±nÄ± tahmin eder.
"""

# =========================
# 1. KÃœTÃœPHANELER
# =========================

import sys
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Veri Ã–n Ä°ÅŸleme
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Regresyon Modelleri
from sklearn.linear_model import Ridge, Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor

# SÄ±nÄ±flandÄ±rma Modelleri
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Metrikler
from sklearn.metrics import (
    r2_score, mean_squared_error,
    accuracy_score, confusion_matrix,
    classification_report, f1_score
)

# =========================
# 2. AYARLAR (CONFIG)
# =========================

RANDOM_STATE = 42
TEST_SIZE = 0.2
PASS_THRESHOLD = 60  # GeÃ§me notu sÄ±nÄ±rÄ±

# AykÄ±rÄ± deÄŸer baskÄ±lamasÄ± yapÄ±lacak sÃ¼tunlar
OUTLIER_COLS = [
    "social_media_hours",
    "netflix_hours",
    "study_hours_per_day"
]

warnings.filterwarnings("ignore")
sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (12, 6)

# =========================
# 3. YARDIMCI FONKSÄ°YONLAR
# =========================

def cap_outliers(df, col):
    """IQR yÃ¶ntemi ile aykÄ±rÄ± deÄŸerleri baskÄ±lar."""
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    low = q1 - 1.5 * iqr
    high = q3 + 1.5 * iqr
    
    # Alt ve Ã¼st sÄ±nÄ±rlarÄ±n dÄ±ÅŸÄ±ndakileri sÄ±nÄ±rlara eÅŸitle (Clipping)
    df[col] = np.clip(df[col], low, high)
    return df

def preprocess_data(df):
    """Veri temizleme, doldurma ve encoding iÅŸlemlerini yapar."""
    print(">> Veri Ã¶n iÅŸleme (Preprocessing) baÅŸlatÄ±ldÄ±...")

    # ID sÃ¼tunu varsa kaldÄ±r
    if "student_id" in df.columns:
        df = df.drop("student_id", axis=1)

    # 1. AykÄ±rÄ± DeÄŸer BaskÄ±lama
    for col in OUTLIER_COLS:
        if col in df.columns:
            df = cap_outliers(df, col)

    # 2. Eksik Veri Doldurma (Mode ile)
    if df["parental_education_level"].isnull().sum() > 0:
        df["parental_education_level"].fillna(
            df["parental_education_level"].mode()[0],
            inplace=True
        )

    # 3. Ordinal Encoding (SÄ±ralÄ± Kategorikler)
    diet_map = {'Poor': 0, 'Fair': 1, 'Good': 2}
    internet_map = {'Poor': 0, 'Average': 1, 'Good': 2}
    edu_map = {'High School': 0, 'Bachelor': 1, 'Master': 2}

    # HaritalamayÄ± uygula (EÄŸer sÃ¼tunlar varsa)
    if "diet_quality" in df.columns: df["diet_quality"] = df["diet_quality"].map(diet_map)
    if "internet_quality" in df.columns: df["internet_quality"] = df["internet_quality"].map(internet_map)
    if "parental_education_level" in df.columns: df["parental_education_level"] = df["parental_education_level"].map(edu_map)

    # 4. Binary Encoding (Evet/HayÄ±r)
    binary_cols = ["part_time_job", "extracurricular_participation"]
    for col in binary_cols:
        if col in df.columns:
            df[col] = df[col].map({'No': 0, 'Yes': 1})

    # 5. One-Hot Encoding (Nominal Kategorikler - Cinsiyet gibi)
    df = pd.get_dummies(df, columns=["gender"], drop_first=True)

    print(">> Ã–n iÅŸleme tamamlandÄ±.")
    return df

def plot_distributions(df):
    """SayÄ±sal deÄŸiÅŸkenlerin daÄŸÄ±lÄ±mÄ±nÄ± Ã§izer."""
    print(">> DaÄŸÄ±lÄ±m grafikleri hazÄ±rlanÄ±yor...")
    num_cols = df.select_dtypes(include=["int64", "float64"]).columns
    
    # Ã‡ok fazla sÃ¼tun varsa hata vermemesi iÃ§in
    cols_to_plot = [c for c in num_cols if c != "exam_score"]
    
    n_cols = 3
    n_rows = (len(cols_to_plot) + n_cols - 1) // n_cols
    
    plt.figure(figsize=(15, 5 * n_rows))
    for i, col in enumerate(cols_to_plot):
        plt.subplot(n_rows, n_cols, i + 1)
        sns.histplot(df[col], kde=True, color="skyblue")
        plt.title(f"{col} DaÄŸÄ±lÄ±mÄ±")
    
    plt.tight_layout()
    plt.show()
    print(">> Grafik kapatÄ±ldÄ±, iÅŸleme devam ediliyor.")

# =========================
# 4. MODEL TUNING MOTORU
# =========================

def run_model_tuning(X_train, y_train, X_test, y_test, models, task="regression"):
    """
    Verilen modeller Ã¼zerinde RandomizedSearchCV ile hiperparametre aramasÄ± yapar.
    """
    results = []
    best_model = None
    best_score = -np.inf
    best_name = ""
    
    # GÃ¶rev tipine gÃ¶re skorlama metriÄŸi seÃ§imi
    scoring_metric = "r2" if task == "regression" else "accuracy"

    print(f"\n{'='*10} {task.upper()} MODELLERÄ° EÄÄ°TÄ°LÄ°YOR {'='*10}")

    for entry in models:
        name = entry["name"]
        model = entry["model"]
        params = entry["params"]

        print(f" -> {name} optimize ediliyor...")

        # HÄ±zlÄ± sonuÃ§ iÃ§in n_iter=10
        search = RandomizedSearchCV(
            model,
            params,
            n_iter=10, 
            cv=3,
            scoring=scoring_metric,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            verbose=0
        )

        search.fit(X_train, y_train)
        best_estimator = search.best_estimator_
        preds = best_estimator.predict(X_test)

        if task == "regression":
            score = r2_score(y_test, preds)
            metric2 = np.sqrt(mean_squared_error(y_test, preds)) # RMSE
            results.append({"Model": name, "R2 Score": score, "RMSE": metric2})
        else:
            score = accuracy_score(y_test, preds)
            metric2 = f1_score(y_test, preds)
            results.append({"Model": name, "Accuracy": score, "F1 Score": metric2})

        # En iyi modeli kaydet
        if score > best_score:
            best_score = score
            best_model = best_estimator
            best_name = name

    # SonuÃ§larÄ± DataFrame'e Ã§evir ve sÄ±rala
    df_results = pd.DataFrame(results).sort_values(
        by="R2 Score" if task == "regression" else "Accuracy", 
        ascending=False
    )

    print(f"\nğŸ† En BaÅŸarÄ±lÄ± {task.capitalize()} Modeli: {best_name} (Skor: {best_score:.4f})")
    return df_results, best_model, best_name

# =========================
# 5. MODEL TANIMLARI
# =========================

def get_regression_models():
    return [
        {"name": "Ridge", "model": Ridge(), "params": {"alpha": [0.1, 1, 10, 100]}},
        {"name": "Lasso", "model": Lasso(), "params": {"alpha": [0.001, 0.01, 0.1, 1]}},
        {"name": "SVR", "model": SVR(), "params": {"C": [0.1, 1, 10], "kernel": ["linear", "rbf"]}},
        {"name": "RandomForest", "model": RandomForestRegressor(random_state=RANDOM_STATE),
         "params": {"n_estimators": [50, 100], "max_depth": [None, 10, 20]}},
        {"name": "XGBoost", "model": XGBRegressor(objective="reg:squarederror", random_state=RANDOM_STATE),
         "params": {"n_estimators": [50, 100], "learning_rate": [0.01, 0.1]}}
    ]

def get_classification_models():
    return [
        {"name": "LogisticRegression", "model": LogisticRegression(solver="liblinear"),
         "params": {"C": [0.1, 1, 10], "penalty": ["l1", "l2"]}},
        {"name": "SVC", "model": SVC(),
         "params": {"C": [0.1, 1, 10], "kernel": ["linear", "rbf"]}},
        {"name": "RandomForest", "model": RandomForestClassifier(random_state=RANDOM_STATE),
         "params": {"n_estimators": [50, 100], "max_depth": [None, 10, 20]}},
        {"name": "XGBoost", "model": XGBClassifier(eval_metric="logloss", random_state=RANDOM_STATE),
         "params": {"n_estimators": [50, 100], "learning_rate": [0.01, 0.1]}}
    ]

# =========================
# 6. ANA PROGRAM (MAIN)
# =========================

def main():
    # 1. Dosya Okuma
    file_name = "student_habits_performance.csv"
    try:
        df = pd.read_csv(file_name)
        print(f">> '{file_name}' baÅŸarÄ±yla yÃ¼klendi. Boyut: {df.shape}")
    except FileNotFoundError:
        print(f"HATA: '{file_name}' dosyasÄ± bulunamadÄ±. LÃ¼tfen dosya yolunu kontrol edin.")
        sys.exit(1)

    # 2. EDA (Ä°steÄŸe baÄŸlÄ± gÃ¶rselleÅŸtirme)
    plot_distributions(df)

    # 3. Ã–n Ä°ÅŸleme
    df = preprocess_data(df)

    # 4. Veri BÃ¶lme (Train/Test)
    X = df.drop("exam_score", axis=1)
    y = df["exam_score"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )

    # 5. Ã–lÃ§eklendirme (Scaling) - Data Leakage Ã¶nlemek iÃ§in split'ten sonra yapÄ±lÄ±r
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # ---------------------------------------------------------
    # GÃ–REV A: REGRESYON (Puan Tahmini)
    # ---------------------------------------------------------
    reg_models = get_regression_models()
    df_reg_results, best_reg_model, best_reg_name = run_model_tuning(
        X_train, y_train, X_test, y_test, reg_models, task="regression"
    )

    print("\n--- Regresyon SonuÃ§ Tablosu ---")
    print(df_reg_results)

    # Regresyon GÃ¶rselleÅŸtirme
    y_pred_reg = best_reg_model.predict(X_test)
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, y_pred_reg, alpha=0.7, color='blue')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.title(f"Regresyon: {best_reg_name} (GerÃ§ek vs Tahmin)")
    plt.xlabel("GerÃ§ek Puan")
    plt.ylabel("Tahmin Edilen Puan")
    plt.show()

    # ---------------------------------------------------------
    # GÃ–REV B: SINIFLANDIRMA (GeÃ§ti/KaldÄ±)
    # ---------------------------------------------------------
    # Hedef deÄŸiÅŸkeni binary formata Ã§evir
    y_train_cls = (y_train >= PASS_THRESHOLD).astype(int)
    y_test_cls = (y_test >= PASS_THRESHOLD).astype(int)

    clf_models = get_classification_models()
    df_clf_results, best_clf_model, best_clf_name = run_model_tuning(
        X_train, y_train_cls, X_test, y_test_cls, clf_models, task="classification"
    )

    print("\n--- SÄ±nÄ±flandÄ±rma SonuÃ§ Tablosu ---")
    print(df_clf_results)

    # SÄ±nÄ±flandÄ±rma GÃ¶rselleÅŸtirme (Confusion Matrix)
    y_pred_cls = best_clf_model.predict(X_test)
    cm = confusion_matrix(y_test_cls, y_pred_cls)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"SÄ±nÄ±flandÄ±rma: {best_clf_name} (Confusion Matrix)")
    plt.xlabel("Tahmin (0: KaldÄ±, 1: GeÃ§ti)")
    plt.ylabel("GerÃ§ek (0: KaldÄ±, 1: GeÃ§ti)")
    plt.show()

    print(f"\n>>> {best_clf_name} DetaylÄ± Rapor:")
    print(classification_report(y_test_cls, y_pred_cls, target_names=['KaldÄ±', 'GeÃ§ti']))

if __name__ == "__main__":
    main()